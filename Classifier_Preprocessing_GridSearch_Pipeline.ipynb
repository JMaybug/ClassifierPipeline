{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, average_precision_score, confusion_matrix\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.inspection import DecisionBoundaryDisplay\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn import tree\n",
    "from sklearn.naive_bayes import CategoricalNB, GaussianNB\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.calibration import LabelEncoder\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loading\n",
    "\n",
    "dataset_name = \"glass.csv\"\n",
    "\n",
    "root_path = \"https://raw.githubusercontent.com/matzim95/ML-datasets/master/\"\n",
    "path_to_data = root_path + dataset_name\n",
    "df = pd.read_csv(path_to_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>refractive index</th>\n",
       "      <th>Sodium</th>\n",
       "      <th>Magnesium</th>\n",
       "      <th>Aluminum</th>\n",
       "      <th>Silicon</th>\n",
       "      <th>Potassium</th>\n",
       "      <th>Calcium</th>\n",
       "      <th>Barium</th>\n",
       "      <th>Iron</th>\n",
       "      <th>Type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>120</td>\n",
       "      <td>1.51652</td>\n",
       "      <td>13.56</td>\n",
       "      <td>3.57</td>\n",
       "      <td>1.47</td>\n",
       "      <td>72.45</td>\n",
       "      <td>0.64</td>\n",
       "      <td>7.96</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>building_windows_non_float_processed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>88</td>\n",
       "      <td>1.51645</td>\n",
       "      <td>13.40</td>\n",
       "      <td>3.49</td>\n",
       "      <td>1.52</td>\n",
       "      <td>72.65</td>\n",
       "      <td>0.67</td>\n",
       "      <td>8.08</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.1</td>\n",
       "      <td>building_windows_non_float_processed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>204</td>\n",
       "      <td>1.51658</td>\n",
       "      <td>14.80</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.99</td>\n",
       "      <td>73.11</td>\n",
       "      <td>0.00</td>\n",
       "      <td>8.28</td>\n",
       "      <td>1.71</td>\n",
       "      <td>0.0</td>\n",
       "      <td>headlamps</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>78</td>\n",
       "      <td>1.51627</td>\n",
       "      <td>13.00</td>\n",
       "      <td>3.58</td>\n",
       "      <td>1.54</td>\n",
       "      <td>72.83</td>\n",
       "      <td>0.61</td>\n",
       "      <td>8.04</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>building_windows_non_float_processed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>76</td>\n",
       "      <td>1.51590</td>\n",
       "      <td>13.02</td>\n",
       "      <td>3.58</td>\n",
       "      <td>1.51</td>\n",
       "      <td>73.12</td>\n",
       "      <td>0.69</td>\n",
       "      <td>7.96</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>building_windows_non_float_processed</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      ID  refractive index  Sodium  Magnesium  Aluminum  Silicon  Potassium  \\\n",
       "119  120           1.51652   13.56       3.57      1.47    72.45       0.64   \n",
       "87    88           1.51645   13.40       3.49      1.52    72.65       0.67   \n",
       "203  204           1.51658   14.80       0.00      1.99    73.11       0.00   \n",
       "77    78           1.51627   13.00       3.58      1.54    72.83       0.61   \n",
       "75    76           1.51590   13.02       3.58      1.51    73.12       0.69   \n",
       "\n",
       "     Calcium  Barium  Iron                                  Type  \n",
       "119     7.96    0.00   0.0  building_windows_non_float_processed  \n",
       "87      8.08    0.00   0.1  building_windows_non_float_processed  \n",
       "203     8.28    1.71   0.0                             headlamps  \n",
       "77      8.04    0.00   0.0  building_windows_non_float_processed  \n",
       "75      7.96    0.00   0.0  building_windows_non_float_processed  "
      ]
     },
     "execution_count": 285,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.sample(frac=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column ID has been removed from analysis.\n",
      "Columns with all unique values:\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "# Delete all-unique ID columns\n",
    "\n",
    "def is_column_unique(col):\n",
    "    return col.nunique() == len(col)\n",
    "\n",
    "unique_columns = df.apply(is_column_unique, axis=0)\n",
    "unique_columns_list = unique_columns[unique_columns].index.tolist()\n",
    "\n",
    "while len(unique_columns_list) > 0:\n",
    "    id_present = ''\n",
    "    id_present = input(f'Found columns with all unique values: {unique_columns_list}. Does the list contain ID column/s? (y/n)')\n",
    "    \n",
    "    if id_present == 'y':\n",
    "        if len(unique_columns_list) == 1:\n",
    "            df.drop(unique_columns_list[0], axis = 1, inplace = True)\n",
    "            print(f\"Column {unique_columns_list[0]} has been removed from analysis.\")\n",
    "        else:    \n",
    "            id_col_name = input(str((f\"Please select the id column, so that it is deleted from analysis: {unique_columns_list}.\")))\n",
    "            df.drop(id_col_name, axis = 1, inplace = True)\n",
    "            print(f\"Column {id_col_name} has been removed from analysis\")\n",
    "    else: break\n",
    "    unique_columns = df.apply(is_column_unique, axis=0)\n",
    "    unique_columns_list = unique_columns[unique_columns].index.tolist()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"Columns with all unique values:\")\n",
    "print(unique_columns_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The list of available columns: ['refractive index', 'Sodium', 'Magnesium', 'Aluminum', 'Silicon', 'Potassium', 'Calcium', 'Barium', 'Iron', 'Type']\n"
     ]
    }
   ],
   "source": [
    "print(\"The list of available columns:\", list(df.columns))\n",
    "\n",
    "label_col = str(input(f'The dataframe has following columns: {list(df.columns)}.\\nSelect the labels (Y) column: '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No missing values found in the database.\n"
     ]
    }
   ],
   "source": [
    "# Dropping NAs\n",
    "\n",
    "# List of common missing value expressions\n",
    "missing_values = ['?', 'n/a', 'NA', \"nan\", 'null', '-', '']\n",
    "\n",
    "# Replace missing value expressions with NaN\n",
    "df.replace(missing_values, np.nan, inplace=True)\n",
    "\n",
    "# Count the number of rows before dropping\n",
    "total_rows_before = df.shape[0]\n",
    "\n",
    "# Drop rows with any NaN values\n",
    "df_dropped = df.dropna()\n",
    "\n",
    "# Count the number of rows after dropping\n",
    "total_rows_after = df_dropped.shape[0]\n",
    "\n",
    "# Calculate the number of rows deleted and its percentage\n",
    "rows_deleted = total_rows_before - total_rows_after\n",
    "percentage_deleted = (rows_deleted / total_rows_before) * 100\n",
    "\n",
    "dropping = ''\n",
    "if rows_deleted > 0:\n",
    "    dropping = input(F\"Warning! The database contains {rows_deleted} rows with MISSING VALUES. It makes {percentage_deleted:.2f}% of the database.\\nWould you like to delete these rows? (y/n)\")\n",
    "    if dropping == 'y':\n",
    "        df = df_dropped.copy()\n",
    "        print(f\"Number of rows deleted: {rows_deleted}\")\n",
    "        print(f\"Percentage of database deleted: {percentage_deleted:.2f}%\")\n",
    "    else:\n",
    "        print('Proceeding without dropping the missing values. The missing values are replaced with zeros.')\n",
    "        df.replace(np.nan, 0, inplace=True)\n",
    "else: print(\"No missing values found in the database.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No duplicate rows found in the database.\n"
     ]
    }
   ],
   "source": [
    "# Dropping duplicates\n",
    "\n",
    "# Count the number of duplicate rows before dropping\n",
    "total_duplicates_before = df.duplicated().sum()\n",
    "\n",
    "# Drop duplicate rows\n",
    "df_deduplicated = df.drop_duplicates()\n",
    "\n",
    "# Count the number of rows after dropping duplicates\n",
    "total_duplicates_after = df.shape[0] - df_deduplicated.shape[0]\n",
    "\n",
    "# Calculate the number of duplicate rows deleted and its percentage\n",
    "if total_duplicates_before > 0:\n",
    "    duplicates_deleted = total_duplicates_before - total_duplicates_after\n",
    "    percentage_duplicates_deleted = (duplicates_deleted / total_duplicates_before) * 100\n",
    "else:\n",
    "    duplicates_deleted = 0\n",
    "    percentage_duplicates_deleted = 0\n",
    "\n",
    "dropping = ''\n",
    "if duplicates_deleted > 0:\n",
    "    dropping = input(f\"Warning! The database contains {duplicates_deleted} DUPLICATE ROWS. It makes {percentage_duplicates_deleted:.2f}% of the database.\\nWould you like to delete these rows? (y/n)\")\n",
    "    if dropping == 'y':\n",
    "        df = df_deduplicated.copy()\n",
    "        print(f\"Number of duplicate rows deleted: {duplicates_deleted}\")\n",
    "        print(f\"Percentage of duplicates deleted: {percentage_duplicates_deleted:.2f}%\")\n",
    "    else:\n",
    "        print('Proceeding without dropping duplicate rows.')\n",
    "else:\n",
    "    print('No duplicate rows found in the database.')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "refractive index    float64\n",
      "Sodium              float64\n",
      "Magnesium           float64\n",
      "Aluminum            float64\n",
      "Silicon             float64\n",
      "Potassium           float64\n",
      "Calcium             float64\n",
      "Barium              float64\n",
      "Iron                float64\n",
      "Type                 object\n",
      "dtype: object\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "# Identify columns containing only integers\n",
    "integer_columns = []\n",
    "for col in df.columns:\n",
    "    if df[col].apply(lambda x: isinstance(x, (int, bool, str)) and re.match(r'^-?\\d+$', str(x)) is not None).all():\n",
    "        integer_columns.append(col)\n",
    "\n",
    "# Convert identified columns to numeric dtype\n",
    "for col in integer_columns:\n",
    "    df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "# Print updated dtypes\n",
    "print(df.dtypes)\n",
    "\n",
    "print(integer_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(label_col, axis=1).copy()\n",
    "y = df[label_col]\n",
    "label_list = list(y.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numeric Columns:\n",
      "['refractive index', 'Sodium', 'Magnesium', 'Aluminum', 'Silicon', 'Potassium', 'Calcium', 'Barium', 'Iron']\n",
      "\n",
      "Categorical Columns:\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "# X encoding\n",
    "\n",
    "threshold_percentage = 0.9  # 90%\n",
    "unique_values_threshold = 6\n",
    "\n",
    "# Step 1: Attempt to convert columns with mixed types to numeric where possible\n",
    "def try_convert_to_numeric(column):\n",
    "    return pd.to_numeric(column, errors='coerce')\n",
    "\n",
    "# Step 2: Determine whether a column is numeric based on the threshold\n",
    "def is_numeric_column(column, threshold_percentage):\n",
    "    column_numeric = try_convert_to_numeric(column)\n",
    "    num_numeric = column_numeric.notna().sum()\n",
    "    return (num_numeric / len(column)) >= threshold_percentage\n",
    "\n",
    "# Step 3: Classify columns and drop non-numeric values from numeric columns\n",
    "numeric_columns = []\n",
    "categorical_columns = []\n",
    "\n",
    "for col in X.columns:\n",
    "    if is_numeric_column(X[col], threshold_percentage):\n",
    "        column_numeric = try_convert_to_numeric(X[col])\n",
    "        if column_numeric.nunique() < unique_values_threshold:\n",
    "            categorical_columns.append(col)\n",
    "        else:\n",
    "            numeric_columns.append(col)\n",
    "            # Drop non-numeric values from numeric columns\n",
    "            X[col] = column_numeric\n",
    "    else:\n",
    "        categorical_columns.append(col)\n",
    "\n",
    "# Print the columns\n",
    "print(\"Numeric Columns:\")\n",
    "print(numeric_columns)\n",
    "\n",
    "print(\"\\nCategorical Columns:\")\n",
    "print(categorical_columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_columns_toformat = []\n",
    "threshold_categories = 15\n",
    "for col in categorical_columns:\n",
    "    if len(list(X[col].unique())) > threshold_categories:\n",
    "        categorical_columns_toformat.append(col)\n",
    "\n",
    "if categorical_columns_toformat:\n",
    "    dropping = ''\n",
    "    dropping = input(f\"Warning! Found {len(categorical_columns_toformat)} categorical columns where the number of categories is higher than {threshold_categories}: {categorical_columns_toformat}. You may need to check the formatting of the columns. Drop the columns for the analysis? (y/n)\")\n",
    "    if dropping == 'y':\n",
    "        X.drop(categorical_columns_toformat, axis=1, inplace=True)\n",
    "\n",
    "        numeric_columns = []\n",
    "        categorical_columns = []\n",
    "\n",
    "        for col in X.columns:\n",
    "            if is_numeric_column(X[col], threshold_percentage):\n",
    "                column_numeric = try_convert_to_numeric(X[col])\n",
    "                if column_numeric.nunique() < unique_values_threshold:\n",
    "                    categorical_columns.append(col)\n",
    "                else:\n",
    "                    numeric_columns.append(col)\n",
    "                    # Drop non-numeric values from numeric columns\n",
    "                    X[col] = column_numeric\n",
    "            else:\n",
    "                categorical_columns.append(col)\n",
    "\n",
    "        # Print the columns\n",
    "        print(\"Numeric Column after dropping:\")\n",
    "        print(numeric_columns)\n",
    "\n",
    "        print(\"\\nCategorical Columns after dropping:\")\n",
    "        print(categorical_columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding categorical variables\n",
    "\n",
    "if categorical_columns:\n",
    "    print(X[categorical_columns].dtypes)\n",
    "    print(\"\\n\")\n",
    "    integer_dtypes = [int, np.int8, np.int16, np.int32, np.int64, \n",
    "                    np.uint, np.uint8, np.uint16, np.uint32, np.uint64,\n",
    "                    float, np.float16, np.float32, np.float64]\n",
    "\n",
    "    for col in categorical_columns:\n",
    "        if X[col].dtype not in integer_dtypes:\n",
    "            # Step 1: Initialize and fit the LabelEncoder\n",
    "            original_labels = X[col].unique()\n",
    "            encoder = LabelEncoder()\n",
    "            X[col] = encoder.fit_transform(X[col])\n",
    "\n",
    "            # Step 2: Extract the original labels and their corresponding encoded values\n",
    "            \n",
    "            encoded_values = list(range(len(original_labels)))\n",
    "\n",
    "            # Step 3: Create and display the reference table\n",
    "            reference_table = dict(zip(original_labels, encoded_values))\n",
    "\n",
    "            print(f\"The values of column {col} are not numerical and have been encoded.\\nReference Table of {col}:\")\n",
    "            for label, encoded in reference_table.items():\n",
    "                print(f\"{label} -> {encoded}\")\n",
    "            print('\\n')\n",
    "        else:\n",
    "            print(f'The labels of column {col} are numerical.')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numerical data has been normalized.\n"
     ]
    }
   ],
   "source": [
    "scaling = ''\n",
    "scaling = input(\"Scale the numerical data using the StandardScaler? (y/n)\")\n",
    "if scaling == \"y\":\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(X[numeric_columns])\n",
    "    X[numeric_columns] = scaler.transform(X[numeric_columns])\n",
    "    print(\"Numerical data has been normalized.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning! The dataset is unbalanced in terms of labels!\n",
      "\n",
      "containers makes 7.94 % of the dataset.\n",
      "building_windows_float_processed makes 6.07 % of the dataset.\n",
      "tableware makes 4.21 % of the dataset.\n",
      "\n",
      "                                       counts  percentage\n",
      "Type                                                    \n",
      "building_windows_non_float_processed      76       35.51\n",
      "building_windows_float_processed          70       32.71\n",
      "headlamps                                 29       13.55\n",
      "vehicle_windows_float_processed           17        7.94\n",
      "containers                                13        6.07\n",
      "tableware                                  9        4.21\n"
     ]
    }
   ],
   "source": [
    "# Checking for label balance\n",
    "\n",
    "try:\n",
    "    label_counts = pd.concat([y.value_counts(), \n",
    "                y.value_counts(normalize=True).mul(100).round(2)],axis=1, keys=('counts','percentage'))\n",
    "except:\n",
    "    print(\"Error, check the labels column.\")\n",
    "\n",
    "unbalanced = False\n",
    "label_low = {}\n",
    "for i in range(len(label_list)):\n",
    "    if label_counts['percentage'].iloc[i] <= 10:\n",
    "        label_low[label_list[i]] = label_counts['percentage'].iloc[i]\n",
    "        unbalanced = True\n",
    "\n",
    "if unbalanced:\n",
    "    print('Warning! The dataset is unbalanced in terms of labels!\\n')\n",
    "    for i in label_low:\n",
    "        print(i,\"makes\",label_low[i],'% of the dataset.')\n",
    "else:\n",
    "    print('The dataset is balanced.')\n",
    "\n",
    "print(\"\\n\",label_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique types in the y array:\n",
      "['str']\n"
     ]
    }
   ],
   "source": [
    "# Create a DataFrame from the array\n",
    "y_types = pd.DataFrame({'values': y})\n",
    "\n",
    "# Add a new column that contains the type of each entry\n",
    "y_types['types'] = y_types['values'].apply(lambda x: type(x).__name__)\n",
    "\n",
    "# Print the unique types\n",
    "unique_types = y_types['types'].unique()\n",
    "print(\"Unique types in the y array:\")\n",
    "print(unique_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The labels are not numerical and have been encoded.\n",
      "\n",
      "Reference Table:\n",
      "building_windows_float_processed -> 0\n",
      "building_windows_non_float_processed -> 1\n",
      "containers -> 2\n",
      "headlamps -> 3\n",
      "tableware -> 4\n",
      "vehicle_windows_float_processed -> 5\n"
     ]
    }
   ],
   "source": [
    "# Labe; encoding\n",
    "# Define the integer types\n",
    "integer_dtypes = [int, np.int8, np.int16, np.int32, np.int64, \n",
    "                  np.uint, np.uint8, np.uint16, np.uint32, np.uint64]\n",
    "\n",
    "# Check if the elements in y are integers\n",
    "if not np.issubdtype(y.dtype, np.integer):\n",
    "    # Step 1: Initialize and fit the LabelEncoder\n",
    "    encoder = LabelEncoder()\n",
    "    y_encoded = encoder.fit_transform(y)\n",
    "\n",
    "    # Step 2: Extract the original labels and their corresponding encoded values\n",
    "    original_labels = encoder.classes_\n",
    "    encoded_values = list(range(len(original_labels)))\n",
    "\n",
    "    # Step 3: Create and display the reference table\n",
    "    reference_table = dict(zip(original_labels, encoded_values))\n",
    "\n",
    "    print(\"The labels are not numerical and have been encoded.\\n\\nReference Table:\")\n",
    "    for label, encoded in reference_table.items():\n",
    "        print(f\"{label} -> {encoded}\")\n",
    "\n",
    "    # Assign encoded labels back to y\n",
    "    y = y_encoded\n",
    "else:\n",
    "    print('The labels are numerical.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The classification task is multiclass.\n"
     ]
    }
   ],
   "source": [
    "unique_classes = np.unique(y)\n",
    "num_unique_classes = len(unique_classes)\n",
    "\n",
    "multiclass = False\n",
    "\n",
    "if num_unique_classes == 2:\n",
    "    print(\"The classification task is binary.\")\n",
    "elif num_unique_classes > 2:\n",
    "    print(\"The classification task is multiclass.\")\n",
    "    multiclass = True\n",
    "else:\n",
    "    print(\"The classification task is not clearly defined (less than 2 unique classes).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[label_col+\" (Label)\"] = y\n",
    "df = X.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_columns.append(label_col + \" (Label)\")\n",
    "\n",
    "for col in categorical_columns:\n",
    "    df[col] = df[col].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved as file: glass.csv\n"
     ]
    }
   ],
   "source": [
    "# Save the summary DataFrame to a CSV file\n",
    "filename = input(\"Save the processed data? If yes, provide the name for csv file: \")\n",
    "\n",
    "if filename:\n",
    "    df.to_csv(f'{filename}.csv', index=False)\n",
    "    print(f\"Data saved as file: {filename}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = StratifiedKFold(n_splits=20, shuffle=True, random_state=42)\n",
    "metric = \"f1\"\n",
    "multiclass_averaging = \"macro\"\n",
    "if multiclass:\n",
    "    metric = metric + \"_\" + multiclass_averaging\n",
    "\n",
    "model_LR = LogisticRegression()\n",
    "\n",
    "model_NB = GaussianNB()\n",
    "\n",
    "model_LDA = LinearDiscriminantAnalysis()\n",
    "\n",
    "model_QDA = QuadraticDiscriminantAnalysis()\n",
    "\n",
    "model_KN = KNeighborsClassifier()\n",
    "KN_grid = {\n",
    "    \"metric\": ['euclidean', 'manhattan', 'cosine'],\n",
    "    \"n_neighbors\": range(1, 31, 2)\n",
    "    }\n",
    "\n",
    "model_DT = DecisionTreeClassifier()\n",
    "DT_grid={\n",
    "        \"max_depth\": range(1,21)\n",
    "        }\n",
    "\n",
    "model_RF = RandomForestClassifier(random_state=42)\n",
    "RF_grid={\n",
    "    'n_estimators': [10, 50, 100, 200],\n",
    "    \"max_depth\": range(2, 16, 2)\n",
    "}\n",
    "\n",
    "model_AB = AdaBoostClassifier(estimator=DecisionTreeClassifier(random_state=42))\n",
    "AB_grid={\n",
    "    'n_estimators': [10, 50, 100, 200],\n",
    "    \"estimator__max_depth\": range(1, 6)\n",
    "}\n",
    "\n",
    "model_GB = GradientBoostingClassifier(random_state=42)\n",
    "GB_grid={\n",
    "        'n_estimators': [10, 50, 100, 200],\n",
    "        \"max_depth\": range(2, 6)\n",
    "    }\n",
    "\n",
    "model_SVC = SVC()\n",
    "SVC_grid={\n",
    "        'C': [0.1, 1, 10, 100],\n",
    "        'kernel': ['linear', 'sigmoid', 'rbf', 'poly'],\n",
    "        'gamma': [0.001, 0.01, 0.1, 1]\n",
    "    }\n",
    "\n",
    "model_grid_dict = {\n",
    "    model_LR: {},  # No hyperparameters for grid search\n",
    "    model_NB: {},  # No hyperparameters for grid search\n",
    "    model_LDA: {},  # No hyperparameters for grid search\n",
    "    model_QDA: {},  # No hyperparameters for grid search\n",
    "    model_KN: KN_grid,\n",
    "    model_DT: DT_grid,\n",
    "    model_RF: RF_grid,\n",
    "    model_AB: AB_grid,\n",
    "    model_GB: GB_grid,\n",
    "    model_SVC: SVC_grid\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_mean = []\n",
    "metric_error = []\n",
    "model_names = []\n",
    "model_params = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_classificator(model, metric, param_grid = False):\n",
    "    if param_grid:\n",
    "        clf = GridSearchCV(\n",
    "            estimator = model,\n",
    "            param_grid = param_grid,\n",
    "            cv=cv,\n",
    "            scoring = metric\n",
    "        )\n",
    "        clf.fit(X, y)\n",
    "        print(f\"\\n\\nGrid search results for {model.__class__.__name__}:\")\n",
    "        print(\"\\tScore of the best parameters:\",round(np.max(clf.cv_results_[\"mean_test_score\"]), 2))\n",
    "        print(\"\\tBest parameters:\",clf.cv_results_[\"params\"][np.argmax(clf.cv_results_[\"mean_test_score\"])])\n",
    "\n",
    "        best_params = clf.best_params_\n",
    "        model.set_params(**best_params)\n",
    "        model_params.append(str(best_params))\n",
    "    else:\n",
    "        model_params.append(np.nan)\n",
    "\n",
    "    pipeline = make_pipeline(StandardScaler(), model)\n",
    "    scores = cross_val_score(pipeline, X, y, cv = cv, scoring = metric)\n",
    "\n",
    "    print(f\"{model.__class__.__name__} cross-validation mean score: {scores.mean().round(3)}+-{(1.96 * scores.std() / np.sqrt(len(scores))).round(3)}\\n\\n\")\n",
    "\n",
    "    metric_mean.append(scores.mean().round(3))\n",
    "    metric_error.append((1.96 * scores.std() / np.sqrt(20)).round(3))\n",
    "    model_names.append(f'{model.__class__.__name__}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [00:00<00:01,  4.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression cross-validation mean score: 0.868+-0.056\n",
      "\n",
      "\n",
      "GaussianNB cross-validation mean score: 1.0+-0.0\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4/10 [00:00<00:01,  5.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearDiscriminantAnalysis cross-validation mean score: 1.0+-0.0\n",
      "\n",
      "\n",
      "QuadraticDiscriminantAnalysis cross-validation mean score: 0.271+-0.185\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5/10 [00:07<00:11,  2.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Grid search results for KNeighborsClassifier:\n",
      "\tScore of the best parameters: 0.93\n",
      "\tBest parameters: {'metric': 'manhattan', 'n_neighbors': 5}\n",
      "KNeighborsClassifier cross-validation mean score: 0.895+-0.046\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 6/10 [00:09<00:08,  2.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Grid search results for DecisionTreeClassifier:\n",
      "\tScore of the best parameters: 0.99\n",
      "\tBest parameters: {'max_depth': 7}\n",
      "DecisionTreeClassifier cross-validation mean score: 0.969+-0.043\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Grid search results for RandomForestClassifier:\n",
      "\tScore of the best parameters: 0.97\n",
      "\tBest parameters: {'max_depth': 6, 'n_estimators': 200}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 7/10 [01:35<01:29, 29.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier cross-validation mean score: 0.962+-0.04\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Grid search results for AdaBoostClassifier:\n",
      "\tScore of the best parameters: 1.0\n",
      "\tBest parameters: {'estimator__max_depth': 3, 'n_estimators': 10}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 8/10 [02:28<01:14, 37.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoostClassifier cross-validation mean score: 1.0+-0.0\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Grid search results for GradientBoostingClassifier:\n",
      "\tScore of the best parameters: 1.0\n",
      "\tBest parameters: {'max_depth': 2, 'n_estimators': 100}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9/10 [06:55<01:49, 109.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GradientBoostingClassifier cross-validation mean score: 1.0+-0.0\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [07:06<00:00, 78.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Grid search results for SVC:\n",
      "\tScore of the best parameters: 0.97\n",
      "\tBest parameters: {'C': 10, 'gamma': 0.001, 'kernel': 'linear'}\n",
      "SVC cross-validation mean score: 0.943+-0.051\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [07:06<00:00, 42.63s/it]\n"
     ]
    }
   ],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "for model, grid in tqdm(model_grid_dict.items()):\n",
    "    run_classificator(model=model, metric=metric, param_grid=grid)\n",
    "\n",
    "warnings.resetwarnings()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    'Model': model_names,\n",
    "    'Parameters': model_params,\n",
    "    'Mean Score': metric_mean,\n",
    "    'Error (+-)': metric_error\n",
    "}\n",
    "\n",
    "# Create a DataFrame from the dictionary\n",
    "df_results = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Parameters</th>\n",
       "      <th>Mean Score</th>\n",
       "      <th>Error (+-)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.868</td>\n",
       "      <td>0.056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LinearDiscriminantAnalysis</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.271</td>\n",
       "      <td>0.185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>KNeighborsClassifier</td>\n",
       "      <td>{'metric': 'manhattan', 'n_neighbors': 5}</td>\n",
       "      <td>0.895</td>\n",
       "      <td>0.046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>DecisionTreeClassifier</td>\n",
       "      <td>{'max_depth': 7}</td>\n",
       "      <td>0.969</td>\n",
       "      <td>0.043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>{'max_depth': 6, 'n_estimators': 200}</td>\n",
       "      <td>0.962</td>\n",
       "      <td>0.040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>{'estimator__max_depth': 3, 'n_estimators': 10}</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>GradientBoostingClassifier</td>\n",
       "      <td>{'max_depth': 2, 'n_estimators': 100}</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>SVC</td>\n",
       "      <td>{'C': 10, 'gamma': 0.001, 'kernel': 'linear'}</td>\n",
       "      <td>0.943</td>\n",
       "      <td>0.051</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           Model  \\\n",
       "0             LogisticRegression   \n",
       "1                     GaussianNB   \n",
       "2     LinearDiscriminantAnalysis   \n",
       "3  QuadraticDiscriminantAnalysis   \n",
       "4           KNeighborsClassifier   \n",
       "5         DecisionTreeClassifier   \n",
       "6         RandomForestClassifier   \n",
       "7             AdaBoostClassifier   \n",
       "8     GradientBoostingClassifier   \n",
       "9                            SVC   \n",
       "\n",
       "                                        Parameters  Mean Score  Error (+-)  \n",
       "0                                              NaN       0.868       0.056  \n",
       "1                                              NaN       1.000       0.000  \n",
       "2                                              NaN       1.000       0.000  \n",
       "3                                              NaN       0.271       0.185  \n",
       "4        {'metric': 'manhattan', 'n_neighbors': 5}       0.895       0.046  \n",
       "5                                 {'max_depth': 7}       0.969       0.043  \n",
       "6            {'max_depth': 6, 'n_estimators': 200}       0.962       0.040  \n",
       "7  {'estimator__max_depth': 3, 'n_estimators': 10}       1.000       0.000  \n",
       "8            {'max_depth': 2, 'n_estimators': 100}       1.000       0.000  \n",
       "9    {'C': 10, 'gamma': 0.001, 'kernel': 'linear'}       0.943       0.051  "
      ]
     },
     "execution_count": 311,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
